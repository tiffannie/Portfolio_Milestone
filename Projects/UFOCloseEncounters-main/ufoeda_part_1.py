# -*- coding: utf-8 -*-
"""UFOEDA_part_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WtBYKiZKaYuHSxY5H7wqCVj7WDNA4B5I
"""

import pandas as pd #data frame operations
import numpy as np #arrays and math functions
from scipy.stats import uniform #for training and test splits
import statsmodels.api as sm  # statistical models (including regression)
import statsmodels.formula.api as smf  # R-like model specification
import matplotlib.pyplot as plt #2D plotting
import seaborn as sns #seaborn for plotting
from sklearn.linear_model import LinearRegression #not used
import os
import sys
import scipy.stats as stats
#plotly graphing
import plotly.graph_objs as go
from plotly import tools
from plotly.offline import iplot, init_notebook_mode
#For Spatial heatmaps
import folium
from folium import Choropleth
from folium.plugins import HeatMap

#read in data
ufo = pd.read_csv("official_ufo_weather_airport.csv")
mov = pd.read_csv("movies.csv")
drug = pd.read_csv("2015alcoholdrug.csv")

ufo = ufo.drop(columns = 'Unnamed: 0')
ufo['datetime']=pd.to_datetime(ufo['datetime'], infer_datetime_format=True)
ufo.dtypes
# ufo.head()

print("What are the dimensions of the dataset?\n",ufo.shape)
print("What are the datatypes for each variable?\n",ufo.dtypes)
# compute descriptive statistics 
print("Descriptive Statistics\n",ufo.describe())
# % of missing.
for col in ufo.columns:
    pct_missing = np.mean(ufo[col].isnull())
    print("Are there any null values?\n",'{} - {}%'.format(col, round(pct_missing*100)))

ufo.describe()

drug = drug.drop([ 'state'], axis=1)
drug = drug.rename(columns={'abb.':'state'})
drug['state']=drug['state'].str.lower()
drop_cols = ['Unnamed: 6','Unnamed: 7','Unnamed: 8','Unnamed: 9','Unnamed: 10','Unnamed: 11','Unnamed: 12']
drug = drug.drop(columns=drop_cols)

print("What are the dimensions of the dataset?\n",drug.shape)
print("What are the datatypes for each variable?\n",drug.dtypes)
# compute descriptive statistics 
print("Descriptive Statistics\n",drug.describe())
# % of missing.
for col in drug.columns:
    pct_missing = np.mean(drug[col].isnull())
    print("Are there any null values?\n",'{} - {}%'.format(col, round(pct_missing*100)))

drug.head()

mov['release_date']=pd.to_datetime(mov['release_date'], infer_datetime_format=True)
mov.sort_values('release_date').reset_index()

# mov.dtypes
# Unnamed: 0                    int64
# id                            int64
# title                        object
# release_date         datetime64[ns]
# original_language            object
# original_title               object
# genre_ids                   float64
# overview                     object

# Drop unnecessary columns
mov = mov.drop(['Unnamed: 0', 'id', 'original_language','original_title', 'genre_ids'], axis=1)
mov = mov[mov.release_date.dt.year <= 2014]


print("What are the dimensions of the dataset?\n",mov.shape)
print("What are the datatypes for each variable?\n",mov.dtypes)
# compute descriptive statistics 
print("Descriptive Statistics\n",mov.describe())
# % of missing.
for col in mov.columns:
    pct_missing = np.mean(mov[col].isnull())
    print("Are there any null values?\n",'{} - {}%'.format(col, round(pct_missing*100)))

##########################################################################
#Starting with movies
plt.hist(mov.release_date.dt.year,density=1, alpha=.7)

mov.columns
# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator #for wordclouds
# for x in mov['overview']:#.astype(str)
#     print(x)
mov['overview'] = mov['overview'].astype(str)
overview_string = " ".join(x for x in mov['overview'])
# wordcloud = WordCloud().generate(' '.join(mov['overview']))

overview_wordcloud = WordCloud(background_color="white",
                               max_words=100,
                               ).generate(overview_string)

plt.subplots(figsize=(20,10))
plt.imshow(overview_wordcloud,interpolation='bilinear')
plt.axis('off')
plt.show()

# read in Movie data
# movies.csv is the file with the combined movie data from API and wikipedia list
mov = pd.read_csv("movies.csv")
# Drop unnecessary columns
# mov = mov.drop([ 'Unnamed: 2', 'Unnamed: 3'], axis=1)
# mov = mov.rename(columns={'Year':'year'})
mov['release_date']=pd.to_datetime(mov['release_date'], infer_datetime_format=True)
mov.sort_values('release_date').reset_index()

# mov.dtypes
# Unnamed: 0                    int64
# id                            int64
# title                        object
# release_date         datetime64[ns]
# original_language            object
# original_title               object
# genre_ids                   float64
# overview                     object

# Drop unnecessary columns
mov = mov.drop(['Unnamed: 0', 'id', 'original_language', 'original_title', 'genre_ids'], axis=1)
mov = mov[mov.release_date.dt.year <= 2019]
mov.head()

mov = mov[mov.release_date.dt.year >= 1960]
mov = mov[mov.release_date.dt.year <= 2014]
#aggregate movies by year and plot total movies per year
movies_released = mov.groupby(mov.release_date.dt.year)['title'].count()
# for x in movies_released:
#     print(x)
print(movies_released)

plt.figure(figsize=(20,15))
plt.plot(movies_released)
plt.xlabel('Year', size=25)
plt.ylabel('Number of Movies Released', size = 25)

mov = mov[mov.release_date.dt.year >= 1960]
mov = mov[mov.release_date.dt.year <= 2014]

import matplotlib.ticker as ticker
au=pd.read_csv("ufo_alcohol.csv")

plt.rcParams.update({'font.size': 18})

fig, ax1 = plt.subplots()
fig.set_size_inches(40, 20, forward=True)

ax1.plot(au["year"], au["ufo_sightings"], 'b-')
ax1.set_xlabel('Year')
ax1.set_ylabel('UFO Sightings', color='b')
ax1.tick_params('y', colors='b')

ax1.xaxis.set_major_locator(ticker.MultipleLocator(2))

for tick in ax1.get_xticklabels():
    tick.set_rotation(45)

ax2 = ax1.twinx()
ax2.plot(movies_released.index,movies_released.values, 'g-')
ax2.set_ylabel('UFO Movie Releases', color='g')
ax2.tick_params('y', colors='g')

plt.show()



from collections import Counter

# split() returns list of all the words in the string
split_it = overview_string.split()

# Pass the split_it list to instance of Counter class.
Counter = Counter(split_it)

# most_common() produces k frequently encountered
# input values and their respective counts.
most_occur = Counter.most_common(10)
print(most_occur)

#aggregate movies by year and plot total movies per year
movies_released = mov.groupby(mov.release_date.dt.year)['title'].count()
movies_released

import time
import datetime

print(ufo['datetime'])

count = 0

movies_in_last_year = []
for sighting_date in ufo['datetime']:
#     if count <= 100:
#         count += 1
    movie_count = 0
    print(sighting_date)
    print( "----------sighting date\n")
    print(count)

    for movie_date in mov['release_date']:

    # if movie premiered before sighting and the movie premiered after 2 years before the sighting
        if movie_date <= sighting_date and movie_date >= (sighting_date - datetime.timedelta(days=730)):
            print(movie_date)
            print( "---------release_date")
            movie_count+=1
    movies_in_last_year.append(movie_count)

print(len(movies_in_last_year))
# movies_in_last_year

print(movies_in_last_year)

# https://seaborn.pydata.org/tutorial/distributions.html
# plt.hist(ufo.datetime,density=1, alpha=.7)

sns.displot(ufo, x='datetime',bins=20)

sns.displot(ufo, x='datetime',kind='kde',bw_adjust=.25)

#the histogram for movies includes movies from like 1902. we'll need to

mov = mov[mov.release_date.dt.year >= 1995]

# https://seaborn.pydata.org/generated/seaborn.lineplot.html
# sns.lineplot(data=movies_released)

# plt.hist(mov.release_date,density=1, alpha=.7)
# mov.release_date
sns.displot(mov,x='release_date',bins=20)

sns.displot(mov,x='release_date',kind='kde',bw_adjust=.25)

from scipy.stats import ttest_ind

# not working
# UFuncTypeError: ufunc 'add' cannot use operands with types dtype('<M8[ns]') and dtype('<M8[ns]')

# stat, p = ttest_ind(mov['release_date'], ufo['datetime'],equal_var = False)

# print('Statistics=%.3f, p=%.3f' % (stat, p))
# # interpret
# alpha = 0.05
# if p > alpha:
#     print('Same distributions (fail to reject H0)')
# else:
#     print('Different distributions (reject H0)')

# try to look at the number of days rather than the date between the last movie released and the next  ufo sighting

# print(movies_in_last_year)
#adding movies released in last two years to dataset
ufo['movies_released'] = movies_in_last_year

ufo.tail()

# df.loc[df['tornado'] ==1, 'Credible'] = 'NO'
# df.loc[df['fog'] ==1, 'Credible'] = 'NO'
# df.loc[df['snow'] ==1, 'Credible'] = 'NO'
# df.loc[df['rain'] ==1, 'Credible'] = 'NO'
# df.loc[df['thunder'] ==1, 'Credible'] = 'NO'
# df.loc[df['hail'] ==1, 'Credible'] = 'NO'
# #df.loc[df['vis'] >= 0.064273, 'Credible'] = 'NO'
# df.loc[df['airport_dist'] <= 1.6, 'Credible'] = 'NO'
# df.loc[df['military_base_dist'] <= 1.6, 'Credible'] = 'NO'
# df.loc[(df.month == 1) & (df.day == 1), "Credible"] = 'NO'
# df.loc[(df.month == 7) & (df.day == 4), "Credible"] = 'NO'

#all the reasons that a sighting was credible or not
ufo.columns
# ufo['datetime']

cols2drop=[
            'datetime'
           ,'duration (seconds)'
           ,'comments'
           ,'latitude'
           ,'longitude'
           ,'time'
           ,'country'
           ,'coord'
           ,'military_base_name'
#            ,'military_base_dist'
           ,'airport_name'
#            ,'airport_dist'
#            ,'temp'
           ,'hail'
#            ,'rain'
#            ,'vis'
#            ,'thunder'
#            ,'fog'
#            ,'tornado'
#            ,'snow'
           ,
          ]
data = ufo.drop(columns=cols2drop)

data.shape

from sklearn import preprocessing

data.dtypes

# city                   object
# state                  object
# shape                  object
# latitude              float64
# longitude             float64
# year                    int64
# month                   int64
# military_base_dist    float64
# airport_dist          float64
# day                     int64
# season                 object
# temp                  float64
# hail                    int64
# rain                    int64
# vis                   float64
# thunder                 int64
# fog                     int64
# tornado                 int64
# snow                    int64
# Credible               object
# movies_released         int64

#you might have to create a new
#datafame all together and smash it with the numerical data once it's encoded.
#yes, this is a whole different approach, but it's your best best since labelencoder doesn't
#want to isolate certain columns

data_to_encode= data[['city','state','shape','year','month','day','season','Credible']]

do_not_encode = data.drop(columns=['city','state','shape','year','month','day','season','Credible'])

print(data_to_encode.head())

print(do_not_encode.head())

le = preprocessing.LabelEncoder()

encoded_data = data_to_encode.apply(le.fit_transform)

encoded_data.head()

#add encoded dataframe and numerical df together
new_data = pd.concat([encoded_data,do_not_encode],axis=1)

new_data.head()

# print(drug.head())
# state = drug['state']
# state = state.to_frame()

# state['state'] = state['state'].astype(str)
# print(type(state))
# drug = drug.drop(columns=['state','2020Election'])

# state = state.apply(le.fit_transform)

# print(state)
# drug['state'] = state

# # print(drug.head())

#before the data was encoded, but now it's fine. these are the actual values
# new_data['movies_released']


#combine drug and ufo data
new_data.head()

test = pd.merge(new_data,drug, how='left',on=['state'])

print(test.tail())

x = set(new_data['rain'])
x

# new_data.to_csv("ufo_wea_airport_movie.csv")
test.to_csv("ufo_wea_airport_movie_drug.csv")

# new_data.corr()
plt.subplots(figsize=(20,15))

sns.heatmap(test.corr(),annot=True)

plt.show()

target = new_data['Credible']

x_df = new_data.drop('Credible',axis=1)

feature_names = x_df.columns

x_df.head()

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

X_train, X_test, y_train, y_test = train_test_split(x_df, target, random_state=42)

#KNN ######################################################################################3


# Loop through different k values to see which has the highest accuracy
# Note: We only use odd numbers because we don't want any ties
train_scores = []
test_scores = []
for k in range(1, 40, 2):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    train_score = knn.score(X_train, y_train)
    test_score = knn.score(X_test, y_test)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(f"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}")
    
    
plt.plot(range(1, 40, 2), train_scores, marker='o')
plt.plot(range(1, 40, 2), test_scores, marker="x")
plt.xlabel("k neighbors")
plt.ylabel("Testing accuracy Score")
plt.show()

# Note that k: 11 seems to be the best choice for this dataset
knn = KNeighborsClassifier(n_neighbors=10)
#knn.fit(X_train, y_train)
knn.fit(x_df, target)
print('k=9 Test Acc: %.3f' % knn.score(X_test, y_test))